{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5a203ce",
   "metadata": {},
   "source": [
    "# 1. Data Exploration & Dataset Verification\n",
    "\n",
    "**Author:** Tim Chinye\n",
    "**Date:** [Current Date]\n",
    "\n",
    "## 1.1. Introduction\n",
    "\n",
    "This notebook serves as the initial step in our project pipeline: understanding and verifying the custom dataset we've collected. Before we can train any models, it is crucial to:\n",
    "\n",
    "1.  **Confirm the dataset structure** and ensure all class directories exist.\n",
    "2.  **Quantify the data** by counting the number of images in each class to check for balance.\n",
    "3.  **Visually inspect** sample images to verify their quality, diversity, and correctness.\n",
    "4.  **Programmatically verify** that all image files are valid and not corrupt.\n",
    "\n",
    "This exploratory data analysis (EDA) ensures we are building our models on a high-quality foundation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581c733",
   "metadata": {},
   "source": [
    "## 1.2. Importing Libraries and Defining Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe3ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "# Define the root path to the original, unprocessed dataset\n",
    "# Assuming the notebook is run from the project's root directory\n",
    "DATASET_ROOT = Path(\"./dataset\")\n",
    "CLASSES = ['rock', 'paper', 'scissors', 'none']\n",
    "\n",
    "# Verify that the root directory exists\n",
    "if not DATASET_ROOT.exists():\n",
    "    print(f\"FATAL: Dataset directory not found at '{DATASET_ROOT.resolve()}'\")\n",
    "    print(\"Please ensure you have run the data collection script or placed the dataset in the correct location.\")\n",
    "else:\n",
    "    print(f\"Dataset directory found at: '{DATASET_ROOT.resolve()}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8cbddf",
   "metadata": {},
   "source": [
    "## 1.3. Quantifying the Dataset\n",
    "\n",
    "Next, we will iterate through each class directory and count the number of images. This is essential to ensure we have a roughly balanced dataset, which prevents the model from becoming biased towards a majority class during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c96d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_counts = {}\n",
    "total_images = 0\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    class_dir = DATASET_ROOT / class_name\n",
    "    if class_dir.is_dir():\n",
    "        count = len([f for f in class_dir.iterdir() if f.is_file()])\n",
    "        image_counts[class_name] = count\n",
    "        total_images += count\n",
    "    else:\n",
    "        image_counts[class_name] = 0\n",
    "        print(f\"Warning: Directory for class '{class_name}' not found.\")\n",
    "\n",
    "# Print the counts\n",
    "print(\"--- Image Counts per Class ---\")\n",
    "for class_name, count in image_counts.items():\n",
    "    print(f\"- {class_name.capitalize():<10}: {count} images\")\n",
    "print(\"------------------------------\")\n",
    "print(f\"Total Images: {total_images}\")\n",
    "\n",
    "# Optional: Plotting the distribution as a bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(image_counts.keys(), image_counts.values(), color=['#FF6347', '#4682B4', '#32CD32', '#6A5ACD'])\n",
    "plt.title('Image Distribution Across Classes')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "for i, count in enumerate(image_counts.values()):\n",
    "    plt.text(i, count + 5, str(count), ha='center') # Add count labels on top of bars\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345f948",
   "metadata": {},
   "source": [
    "## 1.4. Visual Inspection of Sample Images\n",
    "\n",
    "A numerical count is useful, but we must also visually inspect the data. This helps us confirm that the images are high-quality, varied, and correctly labeled. We will display a random selection of 4 images from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe4e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for plotting 4x4 grid (4 classes, 4 samples each)\n",
    "fig, axes = plt.subplots(len(CLASSES), 4, figsize=(12, 12))\n",
    "fig.suptitle('Random Image Samples from Each Class', fontsize=16)\n",
    "\n",
    "for i, class_name in enumerate(CLASSES):\n",
    "    class_dir = DATASET_ROOT / class_name\n",
    "    if not class_dir.is_dir():\n",
    "        continue\n",
    "    \n",
    "    image_files = list(class_dir.glob('*.png'))\n",
    "    if len(image_files) < 4:\n",
    "        print(f\"Warning: Not enough images in '{class_name}' to display 4 samples.\")\n",
    "        sample_images = image_files\n",
    "    else:\n",
    "        sample_images = random.sample(image_files, 4)\n",
    "    \n",
    "    for j, img_path in enumerate(sample_images):\n",
    "        img = mpimg.imread(img_path)\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(class_name)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf07b3",
   "metadata": {},
   "source": [
    "## 1.5. Programmatic Image File Verification\n",
    "\n",
    "Finally, we will run a script to programmatically open and verify every image file in the dataset. A single corrupt image can halt the training process hours in. This step is a crucial safeguard to ensure a smooth model training pipeline.\n",
    "\n",
    "The following code will iterate through all images and report any that cannot be opened or verified by the PIL (Pillow) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_files = []\n",
    "\n",
    "print(\"--- Verifying all images in the dataset... ---\")\n",
    "for class_name in CLASSES:\n",
    "    class_dir = DATASET_ROOT / class_name\n",
    "    if not class_dir.is_dir():\n",
    "        continue\n",
    "    \n",
    "    print(f\"Checking class: {class_name}...\")\n",
    "    for img_path in class_dir.iterdir():\n",
    "        if img_path.is_file() and img_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img.verify()  # Verify that it is a valid image\n",
    "            except (IOError, SyntaxError) as e:\n",
    "                print(f\"  -> CORRUPT FILE DETECTED: {img_path.name} | Reason: {e}\")\n",
    "                corrupt_files.append(str(img_path))\n",
    "\n",
    "print(\"\\n--- Verification Complete ---\")\n",
    "if not corrupt_files:\n",
    "    print(\"âœ… Success! All image files are valid and can be opened.\")\n",
    "else:\n",
    "    print(f\"ðŸš¨ Warning! Found {len(corrupt_files)} corrupt file(s):\")\n",
    "    for f in corrupt_files:\n",
    "        print(f\"  - {f}\")\n",
    "    print(\"\\nThese files should be deleted before proceeding to model training.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
