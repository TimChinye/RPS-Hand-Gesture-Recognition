{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a7963f",
   "metadata": {},
   "source": [
    "# 1. Data Exploration & Preparation\n",
    "\n",
    "This notebook covers the initial exploration and preparation of the Rock-Paper-Scissors dataset. The goal is to understand the data's structure, verify its integrity, and prepare it for the model training pipeline by splitting it into training, validation, and test sets.\n",
    "\n",
    "**Key Steps:**\n",
    "1.  Verify the integrity of the raw images.\n",
    "2.  Visualize sample images from each class.\n",
    "3.  Execute the script to split the final, clean dataset (`dataset_final`) into `train`, `validation`, and `test` directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6620abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "# Assuming the notebook is in the /notebooks directory\n",
    "PROJECT_ROOT = Path.cwd().parent \n",
    "DATASET_DIR = PROJECT_ROOT / \"dataset_final\" # We explore the FINAL, CLEAN dataset\n",
    "CLASSES = ['rock', 'paper', 'scissors', 'none']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4822d0a",
   "metadata": {},
   "source": [
    "## 1.1. Verifying and Visualizing the Dataset\n",
    "\n",
    "Before any training, it's crucial to ensure the dataset is correctly structured and the images are valid. We will count the images in each class and display a few random samples to get a feel for the data's quality and variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd9af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images in each class\n",
    "print(\"Image count per class in the final dataset:\")\n",
    "for class_name in CLASSES:\n",
    "    class_path = DATASET_DIR / class_name\n",
    "    if class_path.is_dir():\n",
    "        num_files = len(list(class_path.glob('*.png')))\n",
    "        print(f\"- {class_name}: {num_files} images\")\n",
    "\n",
    "# Visualize some sample images\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, class_name in enumerate(CLASSES):\n",
    "    class_path = DATASET_DIR / class_name\n",
    "    if class_path.is_dir():\n",
    "        # Get a random image from the class directory\n",
    "        random_image = random.choice(list(class_path.glob('*.png')))\n",
    "        img = mpimg.imread(random_image)\n",
    "        \n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Class: {class_name}\\nShape: {img.shape}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.suptitle(\"Sample Images from the V2 (Cropped) Dataset\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863c667",
   "metadata": {},
   "source": [
    "## 1.2. Splitting the Dataset\n",
    "\n",
    "The dataset is split into `train`, `validation`, and `test` sets to ensure a robust evaluation of the models. This process is handled by the `src/utils/prepare_dataset.py` script, which is executed via `run.py`.\n",
    "\n",
    "The script takes all images from `dataset_final` and distributes them according to a 70/15/15 ratio for training, validation, and testing, respectively. This ensures that the model is trained on a majority of the data, evaluated on a separate validation set during training, and finally tested on a completely unseen test set.\n",
    "\n",
    "We can run this from the command line:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
