{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: The Data Processing and Curation Pipeline\n",
    "\n",
    "**Objective:** To transform a raw, noisy collection of webcam images into a clean, structured, and analysis-ready dataset for training our neural networks.\n",
    "\n",
    "This notebook documents the critical pre-processing steps that were engineered to solve the primary challenge identified in early experiments: **background noise**. The initial models were learning to associate backgrounds with gestures rather than the hand shapes themselves. This pipeline systematically eliminates that noise.\n",
    "\n",
    "The process involves several automated and human-in-the-loop stages:\n",
    "1.  **Data Collection:** Capturing raw images using an interactive tool.\n",
    "2.  **Automated Cropping:** Using MediaPipe to detect and isolate hands.\n",
    "3.  **Manual Review:** A fallback tool for images where auto-cropping fails.\n",
    "4.  **Final Dataset Build:** Consolidating all clean images.\n",
    "5.  **Data Splitting:** Dividing the final dataset into `train`, `validation`, and `test` sets for robust model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Imports and Configuration\n",
    "\n",
    "First, we import the necessary libraries and define the paths for our various data directories. This ensures our pipeline is organized and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import logging\n",
    "import mediapipe as mp\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging for clear output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define project paths relative to the project root\n",
    "PROJECT_ROOT = Path.cwd().parent # Assumes notebook is in 'notebooks' dir\n",
    "RAW_DATASET_DIR = PROJECT_ROOT / \"dataset\"\n",
    "CROPPED_DIR = PROJECT_ROOT / \"dataset_cropped\"\n",
    "REVIEW_DIR = PROJECT_ROOT / \"dataset_review\"\n",
    "FINAL_DIR = PROJECT_ROOT / \"dataset_final\"\n",
    "SPLIT_DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Collection (Conceptual)\n",
    "\n",
    "The first step is to create our own dataset. The brief requires this, and it gives us full control over the data's quality and variability. The script `src/data_collection.py` provides an interactive OpenCV window to capture images from a webcam. \n",
    "\n",
    "**Key Features:**\n",
    "- **Live Camera Feed:** Shows what the camera sees.\n",
    "- **Class Switching:** Use keyboard keys (`r`, `p`, `s`, `n`) to switch the category for the next capture.\n",
    "- **Countdown Capture:** Press `c` to start a 3-second countdown, allowing time to position the hand correctly.\n",
    "- **Organized Saving:** Images are automatically saved into the correct subfolder (e.g., `dataset/rock/`).\n",
    "\n",
    "*This script is interactive and designed to be run from the command line, so we will not execute it here. The output is the raw `dataset/` directory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Automated Hand Cropping\n",
    "\n",
    "This is the core of our solution to the background noise problem. We use Google's **MediaPipe** library, a powerful tool for finding human body landmarks, including hands. \n",
    "\n",
    "The following function, taken directly from `src/utils/auto_crop.py`, performs these actions:\n",
    "1. Initializes MediaPipe's hand detection model.\n",
    "2. Iterates through every image in our raw `dataset/` directory.\n",
    "3. For each image, it attempts to detect a hand.\n",
    "4. If a hand is found, it calculates a bounding box around the hand landmarks.\n",
    "5. It adds a small amount of `PADDING` to the bounding box to ensure the whole hand is included.\n",
    "6. The image is cropped to this bounding box and saved in the `dataset_cropped/` directory, preserving the class subfolder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is from src/utils/auto_crop.py\n",
    "\n",
    "def run_auto_crop(source_dir, dest_dir, padding=16):\n",
    "    \"\"\"Finds hands in the original dataset, crops them with padding, and saves them.\"\"\"\n",
    "    \n",
    "    logger.info(\"Starting Automatic Hand Cropping Pipeline\")\n",
    "    \n",
    "    # Initialize MediaPipe Hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)\n",
    "    \n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    image_count = 0\n",
    "    cropped_count = 0\n",
    "\n",
    "    # Iterate through all class folders in the source directory\n",
    "    for class_path in [p for p in source_dir.iterdir() if p.is_dir()]:\n",
    "        dest_class_path = dest_dir / class_path.name\n",
    "        dest_class_path.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Processing class: {class_path.name}\")\n",
    "        \n",
    "        # Iterate through all images in the class folder\n",
    "        for image_path in class_path.glob(\"*.png\"):\n",
    "            image_count += 1\n",
    "            image = cv2.imread(str(image_path))\n",
    "\n",
    "            if image is None:\n",
    "                logger.warning(f\"Could not read {image_path.name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Process the image to find hands\n",
    "            results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            if results.multi_hand_landmarks:\n",
    "                hand_landmarks = results.multi_hand_landmarks[0]\n",
    "                h, w, _ = image.shape\n",
    "                x_coords = [lm.x * w for lm in hand_landmarks.landmark]\n",
    "                y_coords = [lm.y * h for lm in hand_landmarks.landmark]\n",
    "                \n",
    "                x_min, x_max = int(min(x_coords)), int(max(x_coords))\n",
    "                y_min, y_max = int(min(y_coords)), int(max(y_coords))\n",
    "                    \n",
    "                # Apply padding and crop\n",
    "                x_min = max(0, x_min - padding)\n",
    "                y_min = max(0, y_min - padding)\n",
    "                x_max = min(w, x_max + padding)\n",
    "                y_max = min(h, y_max + padding)\n",
    "                cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "                \n",
    "                if cropped_image.size > 0:\n",
    "                    cv2.imwrite(str(dest_class_path / image_path.name), cropped_image)\n",
    "                    cropped_count += 1\n",
    "            else:\n",
    "                logger.warning(f\"No hand detected in {image_path.name}, skipping.\")\n",
    "                \n",
    "    hands.close()\n",
    "    logger.info(\"Cropping Pipeline Complete\")\n",
    "    logger.info(f\"Total images processed: {image_count}\")\n",
    "    logger.info(f\"Successfully cropped and saved: {cropped_count}\")\n",
    "\n",
    "# Run the function\n",
    "# run_auto_crop(RAW_DATASET_DIR, CROPPED_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Manual Review (Conceptual)\n",
    "\n",
    "Not every image is perfect. Sometimes MediaPipe fails to detect a hand due to poor lighting, awkward angles, or motion blur. The `src/utils/review.py` script is another interactive tool that creates a **human-in-the-loop** workflow to handle these failures.\n",
    "\n",
    "It presents the user with any image that MediaPipe couldn't process and provides options:\n",
    "- **Manually Crop:** Draw a box around the hand to crop it.\n",
    "- **Keep Original:** If the original image is fine as-is (e.g., for the 'none' class).\n",
    "- **Discard & Replace:** Remove the bad image and replace it with a copy of the last known good image to maintain dataset balance.\n",
    "\n",
    "*Like the data collection script, this is an interactive tool not meant for execution within a notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Building the Final Dataset\n",
    "\n",
    "After the automated and manual review steps, our clean images are located in different folders (e.g., `dataset_review/good_crop`, `dataset_review/keep_original`).\n",
    "\n",
    "The `src/utils/build_final_dataset.py` script consolidates all these approved images into a single, clean `dataset_final/` directory. This becomes the master source for our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is from src/utils/build_final_dataset.py\n",
    "\n",
    "def run_build(review_dir, final_dir):\n",
    "    \"\"\"Combines reviewed images into a final dataset directory.\"\"\"\n",
    "    logger.info(\"Building Final Dataset\")\n",
    "    \n",
    "    if final_dir.exists():\n",
    "        shutil.rmtree(final_dir)\n",
    "    final_dir.mkdir()\n",
    "\n",
    "    sources_to_combine = [\n",
    "        review_dir / \"good_crop\",\n",
    "        review_dir / \"keep_original\"\n",
    "    ]\n",
    "\n",
    "    total_copied = 0\n",
    "    for source_base in sources_to_combine:\n",
    "        logger.info(f\"Copying from {source_base.name}...\")\n",
    "        for class_path in [p for p in source_base.iterdir() if p.is_dir()]:\n",
    "            dest_class_path = final_dir / class_path.name\n",
    "            dest_class_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            count = 0\n",
    "            for image_path in class_path.glob(\"*.png\"):\n",
    "                shutil.copy(str(image_path), dest_class_path)\n",
    "                count += 1\n",
    "            total_copied += count\n",
    "            \n",
    "    logger.info(f\"Final Dataset Build Complete. Total images: {total_copied}\")\n",
    "\n",
    "# Run the function\n",
    "# run_build(REVIEW_DIR, FINAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Splitting the Dataset for Training\n",
    "\n",
    "The final step in data preparation is to split our `dataset_final` into training, validation, and test sets. This is crucial for properly training a model and evaluating its ability to generalize to new, unseen data.\n",
    "\n",
    "The `src/utils/prepare_dataset.py` script performs this split:\n",
    "- It shuffles the images within each class to ensure randomness.\n",
    "- It uses a fixed `RANDOM_SEED` to make the split **reproducible**.\n",
    "- It splits the data according to the defined ratios (e.g., 70% train, 15% validation, 15% test).\n",
    "- It copies the files into a new `data/` directory with `train/`, `validation/`, and `test/` subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is from src/utils/prepare_dataset.py\n",
    "\n",
    "def run_split(source_dir, dest_dir, ratios={\"train\": 0.7, \"validation\": 0.15, \"test\": 0.15}, seed=123):\n",
    "    \"\"\"Splits the final dataset into train/validation/test sets.\"\"\"\n",
    "    logger.info(\"Starting Dataset Split\")\n",
    "    random.seed(seed)\n",
    "\n",
    "    if dest_dir.exists():\n",
    "        shutil.rmtree(dest_dir)\n",
    "\n",
    "    # Create destination directories\n",
    "    for split in ratios.keys():\n",
    "        for class_name in [p.name for p in source_dir.iterdir() if p.is_dir()]:\n",
    "            (dest_dir / split / class_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process each class\n",
    "    for class_path in [p for p in source_dir.iterdir() if p.is_dir()]:\n",
    "        image_files = list(class_path.glob(\"*.png\"))\n",
    "        random.shuffle(image_files)\n",
    "        num_images = len(image_files)\n",
    "\n",
    "        # Calculate split points\n",
    "        train_end = int(ratios[\"train\"] * num_images)\n",
    "        validation_end = train_end + int(ratios[\"validation\"] * num_images)\n",
    "        \n",
    "        splits = {\n",
    "            \"train\": image_files[:train_end],\n",
    "            \"validation\": image_files[train_end:validation_end],\n",
    "            \"test\": image_files[validation_end:],\n",
    "        }\n",
    "\n",
    "        # Copy files\n",
    "        for split_name, files in splits.items():\n",
    "            dest_split_dir = dest_dir / split_name / class_path.name\n",
    "            for file_path in files:\n",
    "                shutil.copy(file_path, dest_split_dir)\n",
    "                \n",
    "    logger.info(\"Dataset Split Complete\")\n",
    "\n",
    "# Run the function\n",
    "# run_split(FINAL_DIR, SPLIT_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "At the end of this pipeline, we have successfully transformed our raw webcam captures into a high-quality, organized, and split dataset located in the `data/` directory. This structured data is now perfectly prepared for the model development and training phase, which is covered in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}