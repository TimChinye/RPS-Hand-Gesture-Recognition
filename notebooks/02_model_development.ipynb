{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Model Development and Training\n",
    "\n",
    "**Objective:** To define, compile, and train two distinct Convolutional Neural Network (CNN) architectures to solve our hand gesture recognition problem.\n",
    "\n",
    "With a clean dataset prepared in the previous step, we can now focus on the models. The assessment brief requires us to experiment with two approaches:\n",
    "1.  **A CNN built from scratch:** This establishes a baseline and demonstrates foundational knowledge of neural network architecture.\n",
    "2.  **A model using Transfer Learning:** This leverages a powerful, pre-trained network (MobileNetV2) to achieve higher performance with less data and training time.\n",
    "\n",
    "This notebook will cover:\n",
    "- Setting up data generators with augmentation.\n",
    "- Defining the architecture for both models.\n",
    "- Compiling the models with an optimizer and loss function.\n",
    "- Running the training loop (`.fit()` method).\n",
    "- Saving the final trained models for later evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Imports and Configuration\n",
    "\n",
    "We'll import TensorFlow and other necessary modules. We also define key hyperparameters for training, such as image size, batch size, and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "SAVED_MODELS_DIR = PROJECT_ROOT / \"saved_models\" / \"v2_cropped\"\n",
    "SAVED_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model Hyperparameters\n",
    "IMAGE_SIZE = (150, 150)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "COLOR_MODE = 'rgb'\n",
    "INPUT_SHAPE = IMAGE_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Preparation with Augmentation\n",
    "\n",
    "Before training, we need to create data generators. These tools efficiently load images from disk in batches and feed them to the model. For the training set, we apply **data augmentation**.\n",
    "\n",
    "Data augmentation artificially expands our training dataset by creating modified versions of existing images (rotating, shearing, zooming, flipping). This helps the model become more robust and prevents overfitting by ensuring it learns the general features of a hand gesture, not just the specific pixels of our training images.\n",
    "\n",
    "The validation and test sets are *not* augmented; they are only rescaled, as we need to evaluate the model on unmodified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is from src/utils/image_processing.py\n",
    "\n",
    "def create_data_generators(train_dir, validation_dir, test_dir, image_size, batch_size, color_mode='rgb'):\n",
    "    logger.info(\"Initializing Data Generators\")\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=25,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory=train_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        color_mode=color_mode,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    validation_generator = val_test_datagen.flow_from_directory(\n",
    "        directory=validation_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        color_mode=color_mode,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Create the generators\n",
    "train_generator, validation_generator = create_data_generators(\n",
    "    train_dir=DATA_DIR / \"train\",\n",
    "    validation_dir=DATA_DIR / \"validation\",\n",
    "    test_dir=DATA_DIR / \"test\", # Test generator not needed for training\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "NUM_CLASSES = len(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Model #1 - A CNN from Scratch\n",
    "\n",
    "Our first model is a classic CNN architecture. It consists of a stack of `Conv2D` and `MaxPooling2D` layers that act as feature extractors. These layers learn to identify low-level features like edges and textures, which are combined in deeper layers to form more complex patterns (like fingers and palms).\n",
    "\n",
    "The feature maps are then flattened into a 1D vector and passed through a `Dense` classification head with `Dropout` to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is from src/models/architectures.py\n",
    "def create_scratch_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create, compile, and summarize the model\n",
    "scratch_model = create_scratch_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "scratch_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "scratch_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Scratch Model\n",
    "\n",
    "Now we train the model using the `.fit()` method, passing our training and validation data generators. Keras will handle the training loop, backpropagation, and weight updates for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting training for Model #1: Scratch CNN\")\n",
    "history_scratch = scratch_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = SAVED_MODELS_DIR / \"scratch_model.keras\"\n",
    "scratch_model.save(model_save_path)\n",
    "logger.info(f\"Model #1 saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Model #2 - Transfer Learning with MobileNetV2\n",
    "\n",
    "Transfer learning allows us to use a model that has already been trained on a massive dataset (like ImageNet) and adapt it for our specific task. We use **MobileNetV2** as our base modelâ€”it's lightweight and highly effective for image classification.\n",
    "\n",
    "The process involves:\n",
    "1.  **Instantiating the base model** without its original classification head (`include_top=False`).\n",
    "2.  **Freezing the base model's layers** (`base_model.trainable = False`). This prevents the pre-trained weights from being modified during initial training, preserving the learned features.\n",
    "3.  **Adding a new classification head** on top. We add a `GlobalAveragePooling2D` layer to reduce dimensionality, followed by a `Dense` layer with `softmax` activation for our specific number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is from src/models/architectures.py\n",
    "def create_transfer_model(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(input_shape=input_shape,\n",
    "                             include_top=False,\n",
    "                             weights='imagenet')\n",
    "    base_model.trainable = False # Freeze the base\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create, compile, and summarize the model\n",
    "transfer_model = create_transfer_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "transfer_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Transfer Learning Model\n",
    "\n",
    "The training process is identical to the scratch model. We fit the new model on our data generators. Only the weights of our custom classification head will be updated during this training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting training for Model #2: Transfer Learning (MobileNetV2)\")\n",
    "history_transfer = transfer_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = SAVED_MODELS_DIR / \"transfer_model.keras\"\n",
    "transfer_model.save(model_save_path)\n",
    "logger.info(f\"Model #2 saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We have successfully trained and saved two different models for our hand gesture recognition task. The `history` objects contain the training metrics, and the final model weights are saved to `.keras` files in the `saved_models/` directory. \n",
    "\n",
    "The next and final step is to load these saved models and perform a rigorous evaluation on the unseen test dataset to determine which one performs better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}