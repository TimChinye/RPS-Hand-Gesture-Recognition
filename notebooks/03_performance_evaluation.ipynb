{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Model Performance Evaluation\n",
    "\n",
    "**Objective:** To quantitatively assess the performance of our two trained models on the unseen test dataset and determine the superior architecture.\n",
    "\n",
    "This is the final and most important step in the modeling process. A model is only useful if it can generalize to new data it has never seen before. Here, we will load the saved models from the previous notebook and evaluate them using several key metrics.\n",
    "\n",
    "The evaluation will include:\n",
    "- **Training History Plots:** Visualizing the learning curves for accuracy and loss over epochs.\n",
    "- **Classification Report:** A detailed breakdown of precision, recall, and F1-score for each class.\n",
    "- **Confusion Matrix:** A visual representation of the model's predictions, showing where it gets confused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Imports and Configuration\n",
    "\n",
    "We import the necessary libraries for evaluation, including TensorFlow for loading models, Scikit-learn for metrics, and Matplotlib/Seaborn for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "SAVED_MODELS_DIR = PROJECT_ROOT / \"saved_models\" / \"v2_cropped\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\" / \"v2_cropped\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model Hyperparameters\n",
    "IMAGE_SIZE = (150, 150)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Load Data and Models\n",
    "\n",
    "First, we need to create a data generator for our `test` set. It's crucial that this data is **not augmented** and that `shuffle` is set to `False` so we can correctly align the true labels with the model's predictions.\n",
    "\n",
    "Then, we load the two `.keras` model files we saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data generator (no augmentation, no shuffling)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=DATA_DIR / \"test\",\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load the trained models\n",
    "logger.info(\"Loading saved models...\")\n",
    "scratch_model = tf.keras.models.load_model(SAVED_MODELS_DIR / \"scratch_model.keras\")\n",
    "transfer_model = tf.keras.models.load_model(SAVED_MODELS_DIR / \"transfer_model.keras\")\n",
    "logger.info(\"Models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Evaluation Helper Function\n",
    "\n",
    "To keep our code clean and reusable, we'll use the evaluation function from `src/train.py`. This function automates the entire evaluation process for any given model:\n",
    "\n",
    "1.  It uses the model to `predict` class probabilities for the entire test set.\n",
    "2.  It converts these probabilities into final class predictions using `argmax`.\n",
    "3.  It generates and prints a `classification_report` from Scikit-learn.\n",
    "4.  It generates and plots a `confusion_matrix` using Seaborn.\n",
    "5.  It saves both the report and the matrix plot to the `results/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is from src/train.py\n",
    "\n",
    "def evaluate_model(model, test_generator, save_path_prefix):\n",
    "    \"\"\"Evaluates the model and saves the classification report and confusion matrix.\"\"\"\n",
    "    model_name = Path(save_path_prefix).name\n",
    "    logger.info(f\"Evaluating Model: {model_name}\")\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred_probs = model.predict(test_generator)\n",
    "    y_pred_indices = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true_indices = test_generator.classes\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y_true_indices, y_pred_indices, target_names=class_names)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    report_path = f\"{save_path_prefix}_classification_report.txt\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "    logger.info(f\"Classification report saved to {report_path}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true_indices, y_pred_indices)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    cm_save_path = f\"{save_path_prefix}_confusion_matrix.png\"\n",
    "    plt.savefig(cm_save_path)\n",
    "    logger.info(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Note: The training history plots are generated during the training phase in Notebook 2. The resulting `.png` files are also saved in the `results/` directory.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Evaluating Model #1 (Scratch CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\n",
    "    model=scratch_model,\n",
    "    test_generator=test_generator,\n",
    "    save_path_prefix=str(RESULTS_DIR / \"scratch_model\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Evaluating Model #2 (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\n",
    "    model=transfer_model,\n",
    "    test_generator=test_generator,\n",
    "    save_path_prefix=str(RESULTS_DIR / \"transfer_model\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Final Model Selection\n",
    "\n",
    "Based on the evaluation metrics:\n",
    "\n",
    "- **Scratch Model:** Achieved an accuracy of **78%**. It performed very well on `rock` and `scissors` but struggled with `paper`, often confusing it with `scissors`. This is a strong result for a model trained from scratch on a relatively small dataset, confirming that our data curation pipeline was successful.\n",
    "\n",
    "- **Transfer Learning Model:** Achieved an accuracy of **82%**, meeting the project's success criterion. It showed superior and more balanced performance across all classes, particularly improving the recall for `scissors` and `rock`.\n",
    "\n",
    "**Decision:** The **Transfer Learning (MobileNetV2) model is the clear winner.** It is more accurate, robust, and reliable. This model will be selected as the final model for the interactive game application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}